# -*- coding: utf-8 -*-
"""MLclass04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1USzdu7ifc0VfK8kCwztzDKupMtYaYSNc
"""

import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt
from tensorflow.keras import layers

import pandas as pd
tf.keras.backend.set_floatx('float32')

def build_model(my_inputs, my_outputs, LR):
  # model=tf.keras.models.Sequential()
  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)
  # model.add(tf.keras.layers.Dense(units=1,input_shape=(1,)))
  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=LR),loss="mean_squared_error", metrics=[tf.keras.metrics.RootMeanSquaredError()])
  return model

def train_model(model, dataset , label_name, epochs, batch_size):

  features={name:np.array(value) for name, value in dataset.items()}
  label= np.array(features.pop(label_name))



  history = model.fit(x=features,
                      y=label,
                      batch_size=batch_size,
                      epochs=epochs,shuffle=True)


  trained_weight = model.get_weights()[0][0]
  trained_bias = model.get_weights()[1]

  epochs = history.epoch


  hist = pd.DataFrame(history.history)


  rmse = hist["root_mean_squared_error"]

  return epochs, rmse

print("Train model completed")

def plot_model (trained_weights, trained_bias, feature, label):
  plt.xlabel(feature)
  plt.ylabel(label)

  random_value_df=training_set.sample(n=500)
  plt.scatter(random_value_df[feature],random_value_df[label])

  x0=0
  y0=trained_bias
  x1=random_value_df[feature].max()
  y1=trained_bias+ trained_weights*x1
  plt.plot([x0,x1],[y0,y1],c='b')
  plt.show()

print("model plot")

"""do not run this :- def plot_loss_curve(epochs,mea_training, mea_validation):"""

def plot_loss_curve(epochs,mea_training, mea_validation):
  plt.figure()
  plt.xlabel("epochs")
  plt.ylabel("Error")

  plt.plot(epochs[1:], mea_training[1:], label="Training loss")
  plt.plot(epochs[1:], mea_validation[1:], label="Validation loss")
  plt.legend()

  merged_mae_lists=  mea_training[1:] + mea_validation[1:]
  highest_loss=max( merged_mae_lists)
  lowest_loss=min(merged_mae_lists)
  delta=highest_loss-lowest_loss
  print(delta)

  plt.ylim([lowest_loss-0.05*delta,highest_loss+0.05*delta])
  plt.show()

print("loss function")

def plot_loss_curve(epochs,rmse):
  plt.figure()
  plt.xlabel("epochs")
  plt.ylabel("Error")

  plt.plot(epochs, rmse, label="Training loss")

  plt.legend()


  plt.ylim([rmse.min()*0.9,rmse.max()*1.05])
  plt.show()

print("loss function")

my_features=[ 1.0, 2.0, 3.0,  4.0,  5.0,   6.0,   7.0,   8.0   ,  9.0,  10.0]
mylabel=[3.4,    4.5 ,   3.8,   5.6,   5.8,   7.6 , 10.2 , 11.4 ,   13.6 , 15.1]

training_set= pd.read_csv(filepath_or_buffer="https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv")
testing_set= pd.read_csv(filepath_or_buffer="https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv")

sh_training_set=training_set.reindex(np.random.permutation(training_set.index))

my_features="total_rooms"
mylabel="median_house_value"
print("complete")

testing_set.head()

inputs= {'latitude':tf.keras.layers.Input(shape=(1,),dtype=tf.float32, name='latitude'),
         'longitude': tf.keras.layers.Input(shape=(1,),dtype=tf.float32, name='longitude')}

learning_rate=0.3;
epochs=50;
batch_size=100;
validation_fraction=0.2
# df=sh_training_set
label_name='median_house_value'

preprocessing_layer=tf.keras.layers.Concatenate()(inputs.values())
dense_output= layers.Dense(units=1,name='dense_layers')(preprocessing_layer)
outputs={'dense_output':dense_output}


my_model=build_model(inputs, outputs, learning_rate)

epo, rmse = train_model(my_model, training_set , label_name, epochs, batch_size)

my_model.summary(expand_nested=True)

plot_loss_curve(epo, rmse)

test_features={name:np.array(value) for name, value in testing_set.items()}
test_label=np.array(test_features.pop(label_name))

result= my_model.evaluate(test_features,test_label,batch_size=batch_size)